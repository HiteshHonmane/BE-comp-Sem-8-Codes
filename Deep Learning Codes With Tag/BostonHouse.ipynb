{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "NryIpD_joiP7"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries for array manipulations, model creation, training and visualization\n",
        "import numpy as np\n",
        "from tensorflow.keras.datasets import boston_housing\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard\n",
        "import os\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Boston Housing dataset. It is divided into training and validation datasets\n",
        "(X_train, y_train), (X_valid, y_valid) = boston_housing.load_data()\n"
      ],
      "metadata": {
        "id": "hnoL2LPs7NE5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5397390-cd0b-4272-d96b-a142b590ba7a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/boston_housing.npz\n",
            "57026/57026 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the Sequential model\n",
        "model = Sequential()\n",
        "\n",
        "# Add the first Dense layer with 32 units, input dimension as 13 (number of features in the dataset), and 'relu' activation function\n",
        "model.add(Dense(32, input_dim=13, activation='relu'))\n",
        "\n",
        "# Normalize the activations of the previous layer at each batch, reduces the amount by which the hidden unit values shift around\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "# Add another Dense layer with 16 units and 'relu' activation function\n",
        "model.add(Dense(16, activation='relu'))\n",
        "\n",
        "# Again normalize the activations of the previous layer\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "# Dropout 20% of the neurons to reduce overfitting\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "# Final Dense layer for output. Since it is a regression problem, activation function is linear\n",
        "model.add(Dense(1, activation='linear'))\n"
      ],
      "metadata": {
        "id": "QhtoE28v9Nxd"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Compile the model with mean squared error loss function (best for regression problems) and Adam optimizer (works well in practice and adapts the learning rate based on how training is progressing)\n",
        "model.compile(loss='mean_squared_error', optimizer='adam')\n",
        "\n"
      ],
      "metadata": {
        "id": "xLV4ZgPa7T13"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Set the output directory for saving models and logs\n",
        "output_dir = 'model_output/'\n",
        "run_name = 'regression_baseline'\n",
        "output_path = output_dir + run_name\n",
        "\n",
        "# Create the directory if it does not exist\n",
        "if not os.path.exists(output_path):\n",
        "    os.makedirs(output_path)\n"
      ],
      "metadata": {
        "id": "_VFbIj_M7WuG"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Checkpoint the model weights for every epoch. save_weights_only is set to True because we just need to save the model weights.\n",
        "modelcheckpoint = ModelCheckpoint(output_path + '/weights.{epoch:02d}.hdf5', save_weights_only=True)\n",
        "\n"
      ],
      "metadata": {
        "id": "DFEC6Qd_7aA7"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Create a TensorBoard instance for visualization of training process. It provides insights into how the model loss and accuracy are changing with each epoch\n",
        "tensorboard = TensorBoard(log_dir='logs/' + run_name)\n",
        "\n"
      ],
      "metadata": {
        "id": "LklXhT4d7eL1"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model for a specified number of epochs. An epoch is a complete pass through the entire training dataset.\n",
        "# batch_size determines the number of samples in each mini batch. Its optimal size depends on the data, available memory and the model architecture\n",
        "# validation_data allows the model to see how well it is performing on unseen data\n",
        "model.fit(X_train, y_train,\n",
        "          batch_size=8, epochs=32, verbose=1,\n",
        "          validation_data=(X_valid, y_valid),\n",
        "          callbacks=[modelcheckpoint, tensorboard])  # ModelCheckpoint and TensorBoard callbacks\n"
      ],
      "metadata": {
        "id": "Lcllqduz7g4S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "X_valid[42]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "12WbkUMYBmhV",
        "outputId": "d76882fa-e2f2-417d-c186-8d8b6d38b06d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([  9.32909,   0.     ,  18.1    ,   0.     ,   0.713  ,   6.185  ,\n",
              "        98.7    ,   2.2616 ,  24.     , 666.     ,  20.2    , 396.9    ,\n",
              "        18.13   ])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "y_valid[42]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "66D2dsrXBo2T",
        "outputId": "6fa43991-0769-4b6a-c920-066ef8dc62ca"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "14.1"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict the house price for a validation sample. The input to the predict function should be a two-dimensional array, hence we use np.reshape to reshape the data\n",
        "model.predict(np.reshape(X_valid[42], [1, 13]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FY9xVR8sBNsz",
        "outputId": "1790a18a-bcb4-4c87-bce4-c94b2032b174"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 181ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[15.826309]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    }
  ]
}